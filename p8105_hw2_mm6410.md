P8105 HW2
================
Mahdi Maktabi - mm6410
2024-10-01

This document is P8105 Homework 2.

I loaded necessary packages (e.g. `tidyverse` and `dplyr`).

## Problem 1

I will import the NYC Transit Subway Dataset.

``` r
subway_df = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The dataset has 1,868 observations and 19 variables (1,868 x 19). The
variables include line, station name, station longitude/latitude, routes
1-11, entry and entrance type, vending, and ADA compliance.

To clean the data, the CSV data file was imported, missing values were
excluded, and the variable names were cleaned such that they were all
lowercase. I selected only the variables listed above and changed the
“entry” column so that YES/NO character values were converted to
TRUE/FALSE logistic values.

The current dataset is not tidy. For example, the routes (1-11) can be
condensed into two separate columns: `route_name` and `route_number`.

``` r
distinct(subway_df, station_name, line) |> 
  nrow()
```

    ## [1] 465

``` r
filter(subway_df, ada == TRUE) |> 
  nrow()
```

    ## [1] 468

``` r
filter(subway_df, vending == "NO") |> 
  summarize(mean(entry)) |> 
  pull()
```

    ## [1] 0.3770492

Using the code above, we see that there are:

- 465 distinct stations - identified by name and line
- 468 stations are ADA compliant
- 37.7% of stations entrances/exits without vending allow entrance

The code below will now reformat data to create route_number and
route_name columns:

``` r
subway_df = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |>
  mutate(across(starts_with("route"), as.character))

subway_tidy_df = 
  pivot_longer(
    subway_df,
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    values_drop_na = TRUE
  )
```

``` r
filter(
  subway_tidy_df, route_name == "A") |> 
  nrow()
```

    ## [1] 273

``` r
filter(
  subway_tidy_df, route_name == "A" & ada == TRUE) |> 
  distinct(station_name, line) |> 
  nrow()
```

    ## [1] 17

Using this tidy subway dataset, we can now show that:

- There are 273 distinct subway stations that serve the A train.
- Of the stations that serve the A train, there are 17 that are ADA
  compliant.

## Problem 2

I will import, clean, and merge the datasets: `Mr Trash Wheel`,
`Professor Trash Wheel`, and `Gwynnda Trash Wheel`, into a single
dataset called `trash_wheel_df`.

``` r
mr_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(dumpster:homes_powered) |> 
  slice(1:651) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    homes_powered = as.double(round(homes_powered)),
    year = as.numeric(year),
    name = "mr_trash_wheel"
    )

professor_tw_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", na = c(".", "NA", "")) |>
  janitor::clean_names() |> 
  slice(1:118) |> 
  mutate(
    homes_powered = as.double(round(homes_powered)),
    name = "professor_tw"
    )

gwynnda_tw_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", na = c(".", "NA", "")) |>
  janitor::clean_names() |> 
  slice(1:263) |> 
  mutate(
    homes_powered = as.double(round(homes_powered)),
    name = "gwynnda_tw"
    )

trash_wheel_df = 
  bind_rows(mr_trash_wheel_df, professor_tw_df, gwynnda_tw_df) |> 
  janitor::clean_names() |> 
  relocate(dumpster, name)
```

The merged dataset has 1,032 observations and 15 columns. The variable
names were cleaned, changed to the appropriate variable type, and
excluded any values that were not related to Trash Wheel. Also, I added
an additional variable `name` which will indicate which dataset the data
came from.

``` r
total_weight_professor = 
  trash_wheel_df |> 
  filter(name == "professor_tw") |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))

cig_butts_gwynnda_june_2022 = 
  trash_wheel_df |> 
  filter(
    name == "gwynnda_tw", 
    year == 2022, 
    month == "June") |> 
  summarize(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))
```

- Total weight of trash collected by Professor TW = 247 tons
- Total number of cigarette butts collected by Gwynnda in June 2022 =
  18,120 cigarette butts

## Problem 3

This will be code that will import, clean, and tidy the data from The
Great British Bake Off.

The datasets are: `baker` `bakes` `results` `viewers`

``` r
bakers = read_csv("data/gbb_datasets/bakers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker_first_name", "baker_last_name"), sep = " ", extra = "merge") |> 
  relocate(series)

bakes = read_csv("data/gbb_datasets/bakes.csv", 
                 na = c(".", "NA", "", "N/A", "UNKNOWN", "Unknown")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)
  
results = read_csv("data/gbb_datasets/results.csv", 
                   na = c(".", "NA", ""),
                   skip = 2) |> 
  janitor::clean_names() |> 
  mutate(result = if_else(result == "WINNER", "STAR BAKER", result)) |> 
  rename(baker_first_name = baker)


viewers = read_csv("data/gbb_datasets/viewers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = starts_with("series"),
    names_to = "series",
    values_to = "series_views") |> 
  mutate(series = str_remove(series, "series_"),
         series = as.numeric(series)) |> 
  relocate(series)

gbb_tidy_df = 
  bind_rows(bakers, bakes, results, viewers) |> 
  janitor::clean_names() |> 
  relocate(series, episode)

write_csv(gbb_tidy_df, "data/gbb_datasets/gbb_tidy_df.csv")
```

### Data Cleaning Process

I first imported each data file and did a simple `janitor::cleannames()`
and excluded missing values in order to get a sense of what the data
looked like. Then I began cleaning and tidying each data set
individually to fix elements like cleaning variable names, separating
variables into distinct variables, rearranging the order of variables,
and so on.

One thing that I noticed is that I found myself going back to previous
datasets to add certain things. For example, I had to go back to the
`bakers` dataset in order to specify `baker_first_name` and
`baker_last_name`.

One choice I had to make was not splitting up the `hometown` variable. I
could have added a `shire` variable but ended up not doing it because
there were many of the inputs that did not specify the shire.

The final data set organizes the dataset by series, then episodes, and
then by first name, and last name. I was not sure how to add the last
names after merging. It was difficult because some individuals had the
same first name but different last names, so I was not sure if it would
be possible to differentiate between the two participants.

### Creating Reader-Friendly Star Baker Results

``` r
winners_table = gbb_tidy_df |> 
  filter(tolower(result) == "star baker", series >= 5 & series <= 10) |> 
  select(series, episode, baker_first_name)

pivot_wider(
  winners_table,
  names_from = "episode",
  values_from = "baker_first_name") |> 
  knitr::kable()
```

| series | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
|---:|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|
| 5 | Nancy | Richard | Luis | Richard | Kate | Chetna | Richard | Richard | Richard | Nancy |
| 6 | Marie | Ian | Ian | Ian | Nadiya | Mat | Tamal | Nadiya | Nadiya | Nadiya |
| 7 | Jane | Candice | Tom | Benjamina | Candice | Tom | Andrew | Candice | Andrew | Candice |
| 8 | Steven | Steven | Julia | Kate | Sophie | Liam | Steven | Stacey | Sophie | Sophie |
| 9 | Manon | Rahul | Rahul | Dan | Kim-Joy | Briony | Kim-Joy | Ruby | Ruby | Rahul |
| 10 | Michelle | Alice | Michael | Steph | Steph | Steph | Henry | Steph | Alice | David |

The results show that within each season, the contestants who win
towards the end of the season, end up winning the entire thing. For
example, Nadiya was the star baker in episodes 8 and 9 and ended up as
the winner. Similarly, Sophie was the star baker in episode 9 and won it
in the next episode. One surprising event is that in season 5, Richard
was the star baker in episodes 7, 8, and 9 but Nancy won the season.

### Import, clean, tidy, and organize, viewership data

``` r
viewers = read_csv("data/gbb_datasets/viewers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = starts_with("series"),
    names_to = "series",
    values_to = "series_views") |> 
  mutate(series = str_remove(series, "series_"),
         series = as.numeric(series)) |> 
  relocate(series)

head(viewers, 10) |> 
  knitr::kable()
```

| series | episode | series_views |
|-------:|--------:|-------------:|
|      1 |       1 |         2.24 |
|      2 |       1 |         3.10 |
|      3 |       1 |         3.85 |
|      4 |       1 |         6.60 |
|      5 |       1 |         8.51 |
|      6 |       1 |        11.62 |
|      7 |       1 |        13.58 |
|      8 |       1 |         9.46 |
|      9 |       1 |         9.55 |
|     10 |       1 |         9.62 |

Running this code below will show the average viewers for season 1 and
5.

``` r
viewers_S1avg = viewers |> 
  filter(series == 1) |> 
  pull(series_views) |> 
  mean(na.rm = TRUE)

viewers_s5avg = viewers |> 
  filter(series == 5) |> 
  pull(series_views) |> 
  mean(na.rm = TRUE)
```

- The average viewership for season 1 was = 2.77
- The average viewership for season 5 was = 10.04
