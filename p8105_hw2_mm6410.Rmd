---
title: "P8105 HW2"
author: "Mahdi Maktabi - mm6410"
date: 2024-10-01
output: github_document
---

```{r setup, echo = FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
```

This document is P8105 Homework 2.

I loaded necessary packages (e.g. `tidyverse` and `dplyr`). 

## Problem 1

I will import the NYC Transit Subway Dataset.

```{r, message=FALSE}
subway_df = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The dataset has 1,868 observations and 19 variables (1,868 x 19). The variables include line, station name, station longitude/latitude, routes 1-11, entry and entrance type, vending, and ADA compliance. 

To clean the data, the CSV data file was imported, missing values were excluded, and the variable names were cleaned such that they were all lowercase. I selected only the variables listed above and changed the "entry" column so that YES/NO character values were converted to TRUE/FALSE logistic values.

The current dataset is not tidy. For example, the routes (1-11) can be condensed into two separate columns: `route_name` and `route_number`.


```{r}
distinct(subway_df, station_name, line) |> 
  nrow()

filter(subway_df, ada == TRUE) |> 
  nrow()

filter(subway_df, vending == "NO") |> 
  summarize(mean(entry)) |> 
  pull()
```

Using the code above, we see that there are:

*  465 distinct stations - identified by name and line
*  468 stations are ADA compliant
*  37.7% of stations entrances/exits without vending allow entrance


The code below will now reformat data to create route_number and route_name columns:

```{r, message=FALSE}
subway_df = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |>
  mutate(across(starts_with("route"), as.character))

subway_tidy_df = 
  pivot_longer(
    subway_df,
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    values_drop_na = TRUE
  )
```

```{r, message=FALSE}
filter(
  subway_tidy_df, route_name == "A") |> 
  nrow()

filter(
  subway_tidy_df, route_name == "A" & ada == TRUE) |> 
  distinct(station_name, line) |> 
  nrow()
```

Using this tidy subway dataset, we can now show that:

* There are 273 distinct subway stations that serve the A train.
* Of the stations that serve the A train, there are 17 that are ADA compliant.


## Problem 2

I will import, clean, and merge the datasets: `Mr Trash Wheel`, `Professor Trash Wheel`, and `Gwynnda Trash Wheel`, into a single dataset called `trash_wheel_df`.

```{r merging_datasets, message=FALSE}
mr_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(dumpster:homes_powered) |> 
  slice(1:651) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    homes_powered = as.double(round(homes_powered)),
    year = as.numeric(year),
    name = "mr_trash_wheel"
    )

professor_tw_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", na = c(".", "NA", "")) |>
  janitor::clean_names() |> 
  slice(1:118) |> 
  mutate(
    homes_powered = as.double(round(homes_powered)),
    name = "professor_tw"
    )

gwynnda_tw_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", na = c(".", "NA", "")) |>
  janitor::clean_names() |> 
  slice(1:263) |> 
  mutate(
    homes_powered = as.double(round(homes_powered)),
    name = "gwynnda_tw"
    )

trash_wheel_df = 
  bind_rows(mr_trash_wheel_df, professor_tw_df, gwynnda_tw_df) |> 
  janitor::clean_names() |> 
  relocate(dumpster, name)
```

The merged dataset has 1,032 observations and 15 columns. The variable names were cleaned, changed to the appropriate variable type, and excluded any values that were not related to Trash Wheel. Also, I added an additional variable `name` which will indicate which dataset the data came from.


```{r, message=FALSE}
total_weight_professor = 
  trash_wheel_df |> 
  filter(name == "professor_tw") |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))

cig_butts_gwynnda_june_2022 = 
  trash_wheel_df |> 
  filter(
    name == "gwynnda_tw", 
    year == 2022, 
    month == "June") |> 
  summarize(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))
```

* Total weight of trash collected by Professor TW = 247 tons
* Total number of cigarette butts collected by Gwynnda in June 2022 = 18,120 cigarette butts


## Problem 3

This will be code that will import, clean, and tidy the data from The Great British Bake Off.

The datasets are: `baker` `bakes` `results` `viewers`

```{r, message = FALSE}
bakers = read_csv("data/gbb_datasets/bakers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker_first_name", "baker_last_name"), sep = " ", extra = "merge") |> 
  relocate(series)

bakes = read_csv("data/gbb_datasets/bakes.csv", 
                 na = c(".", "NA", "", "N/A", "UNKNOWN", "Unknown")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)
  
results = read_csv("data/gbb_datasets/results.csv", 
                   na = c(".", "NA", ""),
                   skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)


```

