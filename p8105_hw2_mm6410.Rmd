---
title: "P8105 HW2"
author: "Mahdi Maktabi - mm6410"
date: 2024-10-01
output: github_document
---

```{r setup, echo = FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
```

This document is P8105 Homework 2.

I loaded necessary packages (e.g. `tidyverse` and `dplyr`). 

## Problem 1

I will import the NYC Transit Subway Dataset.

```{r, message=FALSE}
subway_df = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The dataset has 1,868 observations and 19 variables (1,868 x 19). The variables include line, station name, station longitude/latitude, routes 1-11, entry and entrance type, vending, and ADA compliance. 

To clean the data, the CSV data file was imported, missing values were excluded, and the variable names were cleaned such that they were all lowercase. I selected only the variables listed above and changed the "entry" column so that YES/NO character values were converted to TRUE/FALSE logistic values.

The current dataset is not tidy. For example, the routes (1-11) can be condensed into two separate columns: `route_name` and `route_number`.


```{r}
distinct(subway_df, station_name, line) |> 
  nrow()

filter(subway_df, ada == TRUE) |> 
  nrow()

filter(subway_df, vending == "NO") |> 
  summarize(mean(entry)) |> 
  pull()
```

Using the code above, we see that there are:

*  465 distinct stations - identified by name and line
*  468 stations are ADA compliant
*  37.7% of stations entrances/exits without vending allow entrance


The code below will now reformat data to create route_number and route_name columns:

```{r, message=FALSE}
subway_df = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |>
  mutate(across(starts_with("route"), as.character))

subway_tidy_df = 
  pivot_longer(
    subway_df,
    cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    values_drop_na = TRUE
  )
```

```{r, message=FALSE}
filter(
  subway_tidy_df, route_name == "A") |> 
  nrow()

filter(
  subway_tidy_df, route_name == "A" & ada == TRUE) |> 
  distinct(station_name, line) |> 
  nrow()
```

Using this tidy subway dataset, we can now show that:

* There are 273 distinct subway stations that serve the A train.
* Of the stations that serve the A train, there are 17 that are ADA compliant.


## Problem 2

I will import, clean, and merge the datasets: `Mr Trash Wheel`, `Professor Trash Wheel`, and `Gwynnda Trash Wheel`, into a single dataset called `trash_wheel_df`.

```{r merging_datasets, message=FALSE}
mr_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  select(dumpster:homes_powered) |> 
  slice(1:651) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    homes_powered = as.double(round(homes_powered)),
    year = as.numeric(year),
    name = "mr_trash_wheel"
    )

professor_tw_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", na = c(".", "NA", "")) |>
  janitor::clean_names() |> 
  slice(1:118) |> 
  mutate(
    homes_powered = as.double(round(homes_powered)),
    name = "professor_tw"
    )

gwynnda_tw_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", na = c(".", "NA", "")) |>
  janitor::clean_names() |> 
  slice(1:263) |> 
  mutate(
    homes_powered = as.double(round(homes_powered)),
    name = "gwynnda_tw"
    )

trash_wheel_df = 
  bind_rows(mr_trash_wheel_df, professor_tw_df, gwynnda_tw_df) |> 
  janitor::clean_names() |> 
  relocate(dumpster, name)
```

The merged dataset has 1,032 observations and 15 columns. The variable names were cleaned, changed to the appropriate variable type, and excluded any values that were not related to Trash Wheel. Also, I added an additional variable `name` which will indicate which dataset the data came from.


```{r, message=FALSE}
total_weight_professor = 
  trash_wheel_df |> 
  filter(name == "professor_tw") |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))

cig_butts_gwynnda_june_2022 = 
  trash_wheel_df |> 
  filter(
    name == "gwynnda_tw", 
    year == 2022, 
    month == "June") |> 
  summarize(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))
```

* Total weight of trash collected by Professor TW = 247 tons
* Total number of cigarette butts collected by Gwynnda in June 2022 = 18,120 cigarette butts


## Problem 3

This will be code that will import, clean, and tidy the data from The Great British Bake Off.

The datasets are: `baker` `bakes` `results` `viewers`

```{r, message = FALSE}
bakers = read_csv("data/gbb_datasets/bakers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker_first_name", "baker_last_name"), sep = " ", extra = "merge") |> 
  relocate(series)

bakes = read_csv("data/gbb_datasets/bakes.csv", 
                 na = c(".", "NA", "", "N/A", "UNKNOWN", "Unknown")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)
  
results = read_csv("data/gbb_datasets/results.csv", 
                   na = c(".", "NA", ""),
                   skip = 2) |> 
  janitor::clean_names() |> 
  mutate(result = if_else(result == "WINNER", "STAR BAKER", result)) |> 
  rename(baker_first_name = baker)


viewers = read_csv("data/gbb_datasets/viewers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = starts_with("series"),
    names_to = "series",
    values_to = "series_views") |> 
  mutate(series = str_remove(series, "series_"),
         series = as.numeric(series)) |> 
  relocate(series)

gbb_tidy_df = 
  bind_rows(bakers, bakes, results, viewers) |> 
  janitor::clean_names() |> 
  relocate(series, episode)

write_csv(gbb_tidy_df, "data/gbb_datasets/gbb_tidy_df.csv")
```


### Data Cleaning Process

I first imported each data file and did a simple `janitor::cleannames()` and excluded missing values in order to get a sense of what the data looked like. Then I began cleaning and tidying each data set individually to fix elements like cleaning variable names, separating variables into distinct variables, rearranging the order of variables, and so on. 

One thing that I noticed is that I found myself going back to previous datasets to add certain things. For example, I had to go back to the `bakers` dataset in order to specify `baker_first_name` and `baker_last_name`. 

One choice I had to make was not splitting up the `hometown` variable. I could have added a `shire` variable but ended up not doing it because there were many of the inputs that did not specify the shire. 

The final data set organizes the dataset by series, then episodes, and then by first name, and last name. I was not sure how to add the last names after merging. It was difficult because some individuals had the same first name but different last names, so I was not sure if it would be possible to differentiate between the two participants. 


### Creating Reader-Friendly Star Baker Results

```{r}
winners_table = gbb_tidy_df |> 
  filter(tolower(result) == "star baker", series >= 5 & series <= 10) |> 
  select(series, episode, baker_first_name)

pivot_wider(
  winners_table,
  names_from = "episode",
  values_from = "baker_first_name") |> 
  knitr::kable()
```

The results show that within each season, the contestants who win towards the end of the season, end up winning the entire thing. For example, Nadiya was the star baker in episodes 8 and 9 and ended up as the winner. Similarly, Sophie was the star baker in episode 9 and won it in the next episode. One surprising event is that in season 5, Richard was the star baker in episodes 7, 8, and 9 but Nancy won the season. 

### Import, clean, tidy, and organize, viewership data

```{r, message=FALSE}
viewers = read_csv("data/gbb_datasets/viewers.csv", na = c(".", "NA", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = starts_with("series"),
    names_to = "series",
    values_to = "series_views") |> 
  mutate(series = str_remove(series, "series_"),
         series = as.numeric(series)) |> 
  relocate(series)

head(viewers, 10) |> 
  knitr::kable()
```


Running this code below will show the average viewers for season 1 and 5.

```{r}
viewers_S1avg = viewers |> 
  filter(series == 1) |> 
  pull(series_views) |> 
  mean(na.rm = TRUE)

viewers_s5avg = viewers |> 
  filter(series == 5) |> 
  pull(series_views) |> 
  mean(na.rm = TRUE)
```


* The average viewership for season 1 was = 2.77
* The average viewership for season 5 was = 10.04

